{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepLearning.Environments.CatanEnv import CatanEnv, SelfPlayDistribution, CatanTradingEnv, SelfPlayDistTradingEnv\n",
    "from DeepLearning.PPO import MaskablePPO\n",
    "from DeepLearning.GetActionMask import getActionMask, getActionMaskTrading\n",
    "from DeepLearning.GetObservation import getObservation, getObservationSimplified, getSetupObservationValue, getObservationTrading\n",
    "from Agents.AgentModel import AgentMultiModel\n",
    "from Agents.AgentRandom2 import AgentRandom2\n",
    "from Agents.AgentNoMoves import AgentNoMoves\n",
    "import os\n",
    "\n",
    "os.environ[\"UPDATE_MODELS_UNIFORM\"] = \"False\"\n",
    "os.environ[\"UPDATE_MODELS_DIST\"] = \"False\"\n",
    "os.environ[\"MODEL_NAME\"] = \"None\"\n",
    "os.environ[\"MODEL_1_NAME\"] = \"\"\n",
    "os.environ[\"MODEL_2_NAME\"] = \"\"\n",
    "os.environ[\"MODEL_3_NAME\"] = \"\"\n",
    "\n",
    "env = CatanTradingEnv(trading=True)\n",
    "actionMask = getActionMaskTrading\n",
    "observation = getObservationTrading\n",
    "gamma = 0.99\n",
    "\n",
    "info = {\n",
    "    \"env\": \"CatanEnv\",\n",
    "    \"Timesteps\": \"1M\",\n",
    "    \"Opponents\": \"self.play\",\n",
    "    \"Rewards\": \"Setup, Dense, Bank Trades\"\n",
    "}\n",
    "name = \"TradingBase_PlayerTradingAdded_vpActionsBack\"\n",
    "\n",
    "netArchDict = dict(pi=[128, 128], vf=[128, 128])\n",
    "\n",
    "# model = MaskablePPO(\"MlpPolicy\", env, policy_kwargs=dict(net_arch=netArchDict), gamma=gamma, verbose=1, getActionMask=actionMask, getObservation=observation, info=info, saveName=name,tensorboard_log=\"./tensorboard_logs/\")\n",
    "model=MaskablePPO.load(\"DeepLearning/Models/TradingBase_PlayerTradingAdded_vpActionsBack/TradingBase_PlayerTradingAdded_vpActionsBack_800k.zip\", env=env)\n",
    "model.saveName = name\n",
    "model.learn(total_timesteps=2_000_000, tb_log_name=f\"{name}\")\n",
    "# model.save(\"DeepLearning/Models/TradingBase_PlayerTradingAdded_20Turns/Final\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Trained model using env format, use to debug running environments \n",
    "\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from DeepLearning.Environments.NoSetupEnv import NoSetupDenseRewardEnv, NoSetupEnv\n",
    "from DeepLearning.Environments.SetupEnv import SetupRandomWithRoadsEnv\n",
    "from DeepLearning.Environments.CatanEnv import CatanEnv, CatanTradingEnv\n",
    "from DeepLearning.Environments.SelfPlayEnv import SelfPlayEnv, SelfPlaySetupDotTotalEnv\n",
    "import os\n",
    "from Game.CatanPlayer import PlayerStatsTracker\n",
    "from DeepLearning.PPO import MaskablePPO\n",
    "from Agents.AgentRandom2 import AgentRandom2\n",
    "from Agents.AgentNoMoves import AgentNoMoves\n",
    "from Agents.AgentMCTS import AgentMCTS\n",
    "from tabulate import tabulate\n",
    "from DeepLearning.Stats import headers\n",
    "import pandas as pd\n",
    "import random\n",
    "from CatanData.GameStateViewer import SaveGameStateImage, DisplayImage\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "os.environ[\"UPDATE_MODELS\"] = \"False\"\n",
    "os.environ[\"MODEL_NAME\"] = \"None\"\n",
    "\n",
    "model=MaskablePPO.load(\"DeepLearning/Models/DummyTrading.zip\")\n",
    "\n",
    "rewardList = []\n",
    "winner = [0,0,0,0]\n",
    "\n",
    "players = [ AgentRandom2(\"P0\", 0, playerTrading=True),\n",
    "            AgentRandom2(\"P1\", 1, playerTrading=True),\n",
    "            AgentRandom2(\"P2\", 2, playerTrading=True),\n",
    "            AgentRandom2(\"P3\", 3, playerTrading=True),]\n",
    "\n",
    "env = CatanTradingEnv()\n",
    "\n",
    "total_actions = 0\n",
    "\n",
    "for episode in range(1):\n",
    "    done = False\n",
    "    state, info = env.reset()#players=players)\n",
    "\n",
    "    while done != True:\n",
    "        action_masks = get_action_masks(env)\n",
    "        action, _states = model.predict(state, action_masks=action_masks)\n",
    "        state, reward, done, _, info = env.step(action.item())\n",
    "        rewardList.append(reward)\n",
    "        total_actions += 1\n",
    "\n",
    "    winner[env.game.gameState.winner] += 1\n",
    "    \n",
    "    # env.game.gameState.players[0].generatePlayerStats()\n",
    "    # env.game.gameState.players[3].generatePlayerStats()\n",
    "\n",
    "    # print(env.game.gameState.players[0].stats)\n",
    "    # DisplayImage(env.game.gameState)\n",
    "    # print(rewardList[-1] - 25)\n",
    "# print(rewardList)\n",
    "# print(sum(rewardList))\n",
    "\n",
    "print(f\"TotalActions:{total_actions}\")\n",
    "print(\"\\n\\nWinnings: \", winner)\n",
    "\n",
    "\n",
    "\n",
    "# Brick, ore, wool, wheat, wood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Models v Opponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentName             WinRate    MarginError    numTurns    victoryPoints    numRoadsBuilt    devCardsBought  usedDevCards                         settlementsBuilt    citiesBuilt    devCardVP    largestArmy    longestRoad  resourcesReceived                          totalResourcesReceivedPerTurn    totalResourcesDiscarded    totalResourcesStolen  resourcesFromDevCard                   totalResourcesFromDevCard  resourcesFromBankTrade                finalResourceProduction              finalTradeRates                                                                                    setupResourceProduction                totalSetupResourceProduction  setupTradeRates                        setupResourceDiversity    turnsForFirstSettlement    noSettlementsBuilt    turnsForFirstCity    noCitysBuilt    numRoadsFor1stSettlement    totalResourcesFromBankTrade    goodSettlementBankTrades    badSettlementBankTrades    goodCityBankTrades    badCityBankTrades    goodRoadBankTrades    badRoadBankTrades    goodDevCardBankTrades    badDevCardBankTrades    neutralBankTrades    acceptedTrades    rejectedTrades    goodSettlementAcceptedTrades    badSettlementAcceptedTrades    goodCityAcceptedTrades    badCityAcceptedTrades    goodRoadAcceptedTrades    badRoadAcceptedTrades    goodDevCardAcceptedTrades    badDevCardAcceptedTrades    neutralAcceptedTrades    totalPlayerTrades    acceptedPlayerTrades    goodSettlementPlayerTrades    badSettlementPlayerTrades    goodCityPlayerTrades    badCityPlayerTrades    goodRoadPlayerTrades    badRoadPlayerTrades    goodDevCardPlayerTrades    badDevCardPlayerTrades    neutralPlayerTrades\n",
      "------------------  ---------  -------------  ----------  ---------------  ---------------  ----------------  ---------------------------------  ------------------  -------------  -----------  -------------  -------------  ---------------------------------------  -------------------------------  -------------------------  ----------------------  -----------------------------------  ---------------------------  ------------------------------------  -----------------------------------  -------------------------------------------------------------------------------------------------  -----------------------------------  ------------------------------  -----------------------------------  ------------------------  -------------------------  --------------------  -------------------  --------------  --------------------------  -----------------------------  --------------------------  -------------------------  --------------------  -------------------  --------------------  -------------------  -----------------------  ----------------------  -------------------  ----------------  ----------------  ------------------------------  -----------------------------  ------------------------  -----------------------  ------------------------  -----------------------  ---------------------------  --------------------------  -----------------------  -------------------  ----------------------  ----------------------------  ---------------------------  ----------------------  ---------------------  ----------------------  ---------------------  -------------------------  ------------------------  ---------------------\n",
      "Player0                 0.828           2.34     41.372           9.683            12.304            7.856    [4.207, 0.481, 0.554, 0.573, 0.0]             2.283          1.377        1.65          0.731          0.754     [23.357, 19.274, 29.041, 29.62, 31.16]                             3.201                    35.639                   7.896   [0.554, 1.764, 0.0, 0.0, 0.0]                              2.318  [6.5, 0.55, 2.741, 2.031, 8.071]      [7.149, 7.241, 9.132, 9.783, 9.889]  [3.979, 3.981, 3.977, 3.973, 3.971]                                                                [3.302, 3.009, 4.315, 4.351, 4.467]                              19  [3.99, 3.994, 3.992, 3.994, 3.986]                    3.828                      14.9095                 0.098             15.0181         0.208                        4.67346                         19.893                     1.074                      0.962                0.079                 0.562                  6.477               0.846                    0.788                  0.324                 10.168            22.763             80.981                          1                             0.742                     0.853                   0.208                       5.087                   0.903                        2.529                      0.506                     11.09                11.129                   6.964                       0.305                        0.182                  0.007                   0.03                     3.016                 0.089                       0.339                    0.025                   3.107\n",
      "Player0LossesStats     -1              -1        73.9302          7.24419          13.8372           8.00581  [4.616, 0.436, 0.61, 0.68, 0.0]               1.99419        1.18023      1.29651       0.436047       0.284884  [44.547, 29.89, 54.64, 46.349, 61.866]                             3.21                     78.8663                 13.3488  [0.61, 2.174, 0.0, 0.0, 0.0]                               2.784  [14.89, 1.122, 6.471, 3.983, 18.151]  [6.552, 4.512, 7.68, 7.128, 9.459]   [3.994186046511628, 3.9825581395348837, 3.994186046511628, 3.994186046511628, 3.9709302325581395]  [3.616, 2.413, 4.291, 3.663, 4.797]                              19  [3.994, 3.983, 3.994, 3.994, 3.971]                   3.46512                    17.1295                 0.25              19.9619         0.273256                     4.94323                         44.617                     2.34302                    3.02907              0.162791              1.26163               15.7442              1.84302                  1.37791                0.761628              21.4535           44.5523           144.657                          2.2907                        2.02326                   1.40116                 0.360465                   13.6919                  1.51744                      4.38953                    0.697674                  18.9419              25.9826                 16.5233                      0.796512                     0.598837               0.0232558               0.139535                 7.86047               0.290698                    0.55814                  0.127907                5.84302\n",
      "Player1                 0.052           1.38     40.774           3.713             8.576            2.384    [1.275, 0.14, 0.198, 0.172, 0.0]              2.284          0.307        0.511         0.055          0.069     [11.025, 11.405, 15.475, 14.521, 15.65]                            1.67                      8.166                   6.045   [0.345, 0.154, 0.146, 0.106, 0.111]                        0.862  [2.727, 2.618, 2.228, 2.47, 2.505]    [2.754, 3.165, 3.982, 3.946, 3.916]  [3.431, 3.473, 3.451, 3.445, 3.421]                                                                [1.964, 2.106, 2.707, 2.726, 2.727]                              12  [3.536, 3.573, 3.565, 3.552, 3.541]                   2.876                      10.6685                 0.702              6.82342        0.794                        2.37367                         12.548                     0.404                      0.183                0.092                 0.285                  2.513               0.251                    0.826                  0.693                  7.705            33.169             33.562                          0.832                         0.505                     0.314                   0.159                       4.125                   1.364                        2.504                      1.919                     21.962               68.534                  39.222                       0.842                        0.614                  0.233                   0.428                    4.86                  1.324                       2.471                    2.268                  26.767\n",
      "\n",
      "Num turns: 46\n",
      "\n",
      "\n",
      "Winnings:  [828, 52, 64, 56]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Running Agent simulations\n",
    "\"\"\"\n",
    "from Agents.AgentRandom2 import AgentRandom2\n",
    "from Agents.AgentMCTS import AgentMCTS\n",
    "from Agents.AgentUCT import AgentUCT\n",
    "from Agents.AgentNoMoves import AgentNoMoves\n",
    "from Agents.AgentModel import AgentMultiModel, AgentModel\n",
    "from Game.CatanGame import *\n",
    "from CatanSimulator import CreateGame\n",
    "from DeepLearning.PPO import MaskablePPO\n",
    "from Game.CatanPlayer import PlayerStatsTracker\n",
    "from tabulate import tabulate\n",
    "from DeepLearning.Stats import headers\n",
    "import dill as pickle\n",
    "import pandas as pd\n",
    "from CatanData.GameStateViewer import SaveGameStateImage, DisplayImage\n",
    "import time\n",
    "import math\n",
    "from DeepLearning.GetObservation import getObservationSimplified\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "# best_model = AgentMultiModel(\"P1\", 1, model=MaskablePPO.load(\"DeepLearning/Models/NoSetup/NoSetupDenseRewardEnv-10M.zip\"), setupModel=MaskablePPO.load(\"DeepLearning/Models/Setup/SetupRandom_wins_1M.zip\"), fullSetup=False)\n",
    "\n",
    "winner = [0,0,0,0]\n",
    "player0Stats = PlayerStatsTracker()\n",
    "Player0LosingStats = PlayerStatsTracker()\n",
    "player1Stats = PlayerStatsTracker()\n",
    "\n",
    "players = [ AgentModel(\"P0\", 0, recordStats=True, playerTrading=True, model=MaskablePPO.load(\"DeepLearning/Models/Trading_20Turns_CitySettlement_SmallTrading/Trading_20Turns_CitySettlement_SmallTrading_20480.zip\")),\n",
    "            AgentRandom2(\"P1\", 1, recordStats=True, playerTrading=True),\n",
    "            AgentRandom2(\"P2\", 2, recordStats=True, playerTrading=True),\n",
    "            AgentRandom2(\"P3\", 3, recordStats=True, playerTrading=True),]\n",
    "\n",
    "COLLECT_STATS = True\n",
    "for episode in range(1000):\n",
    "    game = CreateGame(players)\n",
    "    game = pickle.loads(pickle.dumps(game, -1))\n",
    "    numTurns = 0\n",
    "    while True:\n",
    "        currPlayer = game.gameState.players[game.gameState.currPlayer]\n",
    "\n",
    "        agentAction = currPlayer.DoMove(game)\n",
    "        agentAction.ApplyAction(game.gameState)\n",
    "\n",
    "        if currPlayer.seatNumber == 0 and agentAction.type == 'EndTurn':\n",
    "            # DisplayImage(game.gameState, agentAction)\n",
    "            # time.sleep(1)\n",
    "            numTurns += 1\n",
    "\n",
    "        if game.gameState.currState == \"OVER\": # or numTurns >= 20:\n",
    "            # DisplayImage(game.gameState, agentAction)\n",
    "            break\n",
    "    \n",
    "    # print(\"Winner: \", game.gameState.winner)\n",
    "    winner[game.gameState.winner] += 1\n",
    "    lost = game.gameState.winner != 0\n",
    "\n",
    "    # Stats\n",
    "    if COLLECT_STATS:\n",
    "        game.gameState.players[0].generatePlayerStats()\n",
    "        game.gameState.players[1].generatePlayerStats()\n",
    "\n",
    "        player0Stats += game.gameState.players[0].stats\n",
    "        player1Stats += game.gameState.players[1].stats\n",
    "        if lost:\n",
    "            Player0LosingStats += game.gameState.players[0].stats\n",
    "\n",
    "# Collect stats\n",
    "if COLLECT_STATS:\n",
    "    player0Stats.getAverages()\n",
    "    Player0LosingStats.getAverages()\n",
    "    player1Stats.getAverages()\n",
    "    player0Data = player0Stats.getList()\n",
    "    player0LosingData = Player0LosingStats.getList()\n",
    "    player1Data = player1Stats.getList()\n",
    "\n",
    "    p_hat0 = winner[0] / sum(winner)\n",
    "    p_hat1 = winner[1] / sum(winner)\n",
    "    margin_error0 = round(100*(1.96 * math.sqrt((p_hat0 * (1 - p_hat0)) / sum(winner))), 2)\n",
    "    margin_error1 = round(100*(1.96 * math.sqrt((p_hat1 * (1 - p_hat1)) / sum(winner))), 2)\n",
    "    player0Data.insert(0, margin_error0)\n",
    "    player0LosingData.insert(0, -1)\n",
    "    player1Data.insert(0, margin_error1)\n",
    "    player0Data.insert(0, winner[0]/sum(winner))\n",
    "    player0LosingData.insert(0, -1)\n",
    "    player1Data.insert(0, winner[1]/sum(winner))\n",
    "    player0Data.insert(0, \"Player0\")\n",
    "    player0LosingData.insert(0, \"Player0LossesStats\")\n",
    "    player1Data.insert(0, \"Player1\")\n",
    "\n",
    "    table = tabulate([player0Data, player0LosingData, player1Data], headers=headers, tablefmt='simple')\n",
    "    print(table)\n",
    "\n",
    "print(f\"\\nNum turns: {numTurns}\")\n",
    "\n",
    "print(\"\\n\\nWinnings: \", winner)\n",
    "\n",
    "\n",
    "# Brick, ore, wool, wheat, wood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save to csv\n",
    "# fileName = f'TradingBase_vpActionAdjusted3M_v_3Random.csv'\n",
    "# df = pd.DataFrame([player0Data, player0LosingData, player1Data], columns=headers)\n",
    "# df.to_csv(f'DeepLearning/Data/Trading/{fileName}', index=False)\n",
    "\n",
    "# from DeepLearning.GetActionMask import allActionsDict\n",
    "\n",
    "# print(allActionsDict)\n",
    "\n",
    "# model.save(\"DeepLearning/Models/GammaTest-04/Final\")\n",
    "\n",
    "# model = MaskablePPO.load(\"DeepLearning/Models/Setup_CityThenRoad/Setup_CityThenRoad_133120.zip\")\n",
    "# model.getObservation = getSetupObservationValue\n",
    "# model.save(\"DeepLearning/Models/Setup_CityThenRoad/Setup_CityThenRoad.zip\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
