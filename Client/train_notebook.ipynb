{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "from CatanEnv import CatanSetupEnv\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "\n",
    "env = CatanSetupEnv()\n",
    "\n",
    "model = MaskablePPO(\"MlpPolicy\", env, gamma=0.4, verbose=1) #, tensorboard_log=\"./tensorboard_logs/\")\n",
    "# model.learn(total_timesteps=50_000) #, tb_log_name='simple_setup_phase_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from GameStateViewer import SaveGameStateImage\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from ModelState import getInputState\n",
    "\n",
    "##############################################################\n",
    "rewardList = []\n",
    "rewardList100 = []\n",
    "winner = [0,0,0,0]\n",
    "\n",
    "for episode in range(1):\n",
    "    done = False\n",
    "    state, info = env.reset()\n",
    "\n",
    "    while done != True:\n",
    "        action_masks = get_action_masks(env)\n",
    "        action, _states = model.predict(state, action_masks=action_masks)\n",
    "        state, reward, done, _, info = env.step(action.item())\n",
    "        # print(f\"Reward: {reward+7}\")\n",
    "        rewardList.append(reward)\n",
    "    \n",
    "    # Once initial placement is done use random agents till game is over\n",
    "    while True:\n",
    "        currPlayer = env.game.gameState.players[env.game.gameState.currPlayer]\n",
    "\n",
    "        agentAction = currPlayer.DoMove(env.game)\n",
    "        agentAction.ApplyAction(env.game.gameState)\n",
    "        print(len(getInputState(env.game.gameState)))\n",
    "\n",
    "        if env.game.gameState.currState == \"OVER\":\n",
    "            break\n",
    "    winner[env.game.gameState.winner] += 1\n",
    "\n",
    "\n",
    "print(winner)\n",
    "\n",
    "\n",
    "\n",
    "# plt.plot(rewardList)\n",
    "# plt.xlabel('Episode (100)')\n",
    "# plt.ylabel('Avg VP')\n",
    "# plt.show()\n",
    "\n",
    "# img = mpimg.imread('TRAINING.png')\n",
    "# plt.imshow(img)\n",
    "# plt.axis('off')  # Optional: Remove axes for a clean display\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CatanEnv import CatanSetupEnv, CatanEnv\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "\n",
    "env = CatanEnv()\n",
    "\n",
    "model = MaskablePPO(\"MlpPolicy\", env, gamma=0.4, verbose=1, tensorboard_log=\"./tensorboard_logs/\")\n",
    "model.learn(total_timesteps=500_000, tb_log_name='full_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 26, 15, 32]\n",
      "5.37\n"
     ]
    }
   ],
   "source": [
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from GameStateViewer import SaveGameStateImage\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from ModelState import getInputState\n",
    "\n",
    "##############################################################\n",
    "rewardList = []\n",
    "rewardList100 = []\n",
    "winner = [0,0,0,0]\n",
    "vpList = []\n",
    "\n",
    "for episode in range(100):\n",
    "    done = False\n",
    "    state, info = env.reset()\n",
    "\n",
    "    while done != True:\n",
    "        action_masks = get_action_masks(env)\n",
    "        action, _states = model.predict(state, action_masks=action_masks)\n",
    "        state, reward, done, _, info = env.step(action.item())\n",
    "        # print(f\"Reward: {reward+7}\")\n",
    "        rewardList.append(reward)\n",
    "    \n",
    "    winner[env.game.gameState.winner] += 1\n",
    "    vpList.append(env.players[2].victoryPoints)\n",
    "\n",
    "\n",
    "print(winner)\n",
    "print(sum(vpList)/len(vpList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieldrummond/Catan/PyCatron/TC2/Client/env/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.action_masks to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.action_masks` for environment variables or `env.get_wrapper_attr('action_masks')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2        |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 202      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.16        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 172         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012719718 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.71       |\n",
      "|    explained_variance   | -0.119      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.367       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0519     |\n",
      "|    value_loss           | 1.3         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.16        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 178         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013713958 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.68       |\n",
      "|    explained_variance   | -0.115      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.377       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0577     |\n",
      "|    value_loss           | 1.33        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 179         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013611301 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.65       |\n",
      "|    explained_variance   | -0.176      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.302       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0589     |\n",
      "|    value_loss           | 1.33        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 181         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013174655 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.61       |\n",
      "|    explained_variance   | -0.191      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.287       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0572     |\n",
      "|    value_loss           | 1.37        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.76        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 184         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014436251 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.57       |\n",
      "|    explained_variance   | -0.212      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.346       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0599     |\n",
      "|    value_loss           | 1.36        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 181         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 78          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016689163 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.53       |\n",
      "|    explained_variance   | -0.2        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.45        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0663     |\n",
      "|    value_loss           | 1.35        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 89          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017772939 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.51       |\n",
      "|    explained_variance   | -0.2        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.314       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0654     |\n",
      "|    value_loss           | 1.36        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2          |\n",
      "|    ep_rew_mean          | 0.6        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 185        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 99         |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01917909 |\n",
      "|    clip_fraction        | 0.275      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.45      |\n",
      "|    explained_variance   | -0.327     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.362      |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0728    |\n",
      "|    value_loss           | 1.49       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 1.12        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 109         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019517265 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.42       |\n",
      "|    explained_variance   | -0.217      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.247       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0733     |\n",
      "|    value_loss           | 1.42        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.84        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 119         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022063393 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.37       |\n",
      "|    explained_variance   | -0.331      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.39        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0761     |\n",
      "|    value_loss           | 1.51        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.84        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 130         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022397356 |\n",
      "|    clip_fraction        | 0.296       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.31       |\n",
      "|    explained_variance   | -0.282      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.27        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0765     |\n",
      "|    value_loss           | 1.45        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.96        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 141         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023905424 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.26       |\n",
      "|    explained_variance   | -0.359      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.362       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.076      |\n",
      "|    value_loss           | 1.56        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.8         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 152         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024906646 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.2        |\n",
      "|    explained_variance   | -0.274      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.356       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0755     |\n",
      "|    value_loss           | 1.49        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.8         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 162         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026371865 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.13       |\n",
      "|    explained_variance   | -0.315      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.323       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0766     |\n",
      "|    value_loss           | 1.54        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 172         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026500847 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.11       |\n",
      "|    explained_variance   | -0.277      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.446       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0798     |\n",
      "|    value_loss           | 1.62        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 182         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029489782 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.06       |\n",
      "|    explained_variance   | -0.255      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.343       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0832     |\n",
      "|    value_loss           | 1.61        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 1.16        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 194         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030437956 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3          |\n",
      "|    explained_variance   | -0.268      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.401       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0843     |\n",
      "|    value_loss           | 1.66        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2          |\n",
      "|    ep_rew_mean          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 189        |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 204        |\n",
      "|    total_timesteps      | 38912      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03198877 |\n",
      "|    clip_fraction        | 0.357      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.96      |\n",
      "|    explained_variance   | -0.261     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.178      |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0857    |\n",
      "|    value_loss           | 1.62       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2          |\n",
      "|    ep_rew_mean          | 1.28       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 190        |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 214        |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03250415 |\n",
      "|    clip_fraction        | 0.36       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.91      |\n",
      "|    explained_variance   | -0.287     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.418      |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0871    |\n",
      "|    value_loss           | 1.79       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2          |\n",
      "|    ep_rew_mean          | 0.92       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 191        |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 224        |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03273402 |\n",
      "|    clip_fraction        | 0.373      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.84      |\n",
      "|    explained_variance   | -0.275     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.335      |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0879    |\n",
      "|    value_loss           | 1.73       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 1.4         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 235         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031247325 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | -0.271      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.453       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.083      |\n",
      "|    value_loss           | 1.73        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 1.48        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 244         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032133088 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.72       |\n",
      "|    explained_variance   | -0.26       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.409       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0817     |\n",
      "|    value_loss           | 1.76        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.8         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 255         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033991802 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.69       |\n",
      "|    explained_variance   | -0.235      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.656       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0864     |\n",
      "|    value_loss           | 1.8         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 1.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 265         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034893863 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.63       |\n",
      "|    explained_variance   | -0.303      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.68        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.085      |\n",
      "|    value_loss           | 1.82        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sb3_contrib.ppo_mask.ppo_mask.MaskablePPO at 0x28cff3990>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from DeepLearning.Env import CatanSetupRandomEnv\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "import os\n",
    "os.environ['SAVE_IMAGE'] = 'False'\n",
    "\n",
    "env = CatanSetupRandomEnv()\n",
    "\n",
    "model = MaskablePPO(\"MlpPolicy\", env, gamma=0.4, verbose=1, tensorboard_log=\"./tensorboard_logs/\")\n",
    "model.learn(total_timesteps=100_000, tb_log_name='ENV-SetupRandom_REWARD-win')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General\n",
      " numTurns: 68.88\n",
      " victoryPoints: 8.07\n",
      " finalTradeRates: [3.59, 3.6, 3.58, 3.62, 3.66]\n",
      "Dev Card Breakdown\n",
      " devCardsBought: 4.97\n",
      " usedDevCards: [2.78, 0.27, 0.4, 0.45, 0.0]\n",
      "Point Breakdown\n",
      " settlementsBuilt: 2.09\n",
      " citiesBuilt: 1.73\n",
      " devCardVP: 0.9\n",
      " largestArmy: 0.39\n",
      " longestRoad: 0.39\n",
      "Resource Breakdown\n",
      " resourcesReceived: [35.67, 46.48, 36.78, 50.87, 40.33]\n",
      " totalResourcesDiscarded: 49.18\n",
      " totalResourcesStolen: 12.65\n",
      " resourcesFromDevCard: [0.75, 0.59, 0.31, 0.17, 0.16]\n",
      " resourcesFromBankTrade: [7.53, 7.54, 7.71, 7.36, 8.06]\n",
      " finalResourceProduction: [6.91, 10.29, 8.5, 10.56, 8.87]\n",
      "Setup Breakdown\n",
      " setupResourceProduction: [3.55, 4.23, 3.43, 4.76, 3.8]\n",
      " setupTradeRates: [3.92, 3.96, 3.9, 3.96, 3.96]\n",
      "\n",
      "Winnings:  [48, 20, 17, 15]\n"
     ]
    }
   ],
   "source": [
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from DeepLearning.Env import CatanSetupRandomEnv\n",
    "import os\n",
    "from Game.CatanPlayer import PlayerStatsTracker\n",
    "\n",
    "# os.environ['SAVE_IMAGE'] = 'True'\n",
    "env = CatanSetupRandomEnv()\n",
    "\n",
    "rewardList = []\n",
    "rewardList100 = []\n",
    "winner = [0,0,0,0]\n",
    "vpList = []\n",
    "\n",
    "stats = PlayerStatsTracker()\n",
    "\n",
    "for episode in range(100):\n",
    "    done = False\n",
    "    state, info = env.reset()\n",
    "\n",
    "    while done != True:\n",
    "        action_masks = get_action_masks(env)\n",
    "        action, _states = model.predict(state, action_masks=action_masks)\n",
    "        state, reward, done, _, info = env.step(action.item())\n",
    "        # print(f\"Reward: {reward+7}\")\n",
    "        rewardList.append(reward)\n",
    "\n",
    "    \n",
    "    winner[env.game.gameState.winner] += 1\n",
    "    \n",
    "    env.game.gameState.players[0].generatePlayerStats()\n",
    "    # print(env.game.gameState.players[0].stats)\n",
    "    stats += env.game.gameState.players[0].stats\n",
    "\n",
    "stats.getAverages()\n",
    "print(stats)\n",
    "\n",
    "print(\"Winnings: \", winner)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
