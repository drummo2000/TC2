{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "from CatanEnv import CatanSetupEnv\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "\n",
    "env = CatanSetupEnv()\n",
    "\n",
    "model = MaskablePPO(\"MlpPolicy\", env, gamma=0.4, verbose=1) #, tensorboard_log=\"./tensorboard_logs/\")\n",
    "# model.learn(total_timesteps=50_000) #, tb_log_name='simple_setup_phase_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from GameStateViewer import SaveGameStateImage\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from ModelState import getInputState\n",
    "\n",
    "##############################################################\n",
    "rewardList = []\n",
    "rewardList100 = []\n",
    "winner = [0,0,0,0]\n",
    "\n",
    "for episode in range(1):\n",
    "    done = False\n",
    "    state, info = env.reset()\n",
    "\n",
    "    while done != True:\n",
    "        action_masks = get_action_masks(env)\n",
    "        action, _states = model.predict(state, action_masks=action_masks)\n",
    "        state, reward, done, _, info = env.step(action.item())\n",
    "        # print(f\"Reward: {reward+7}\")\n",
    "        rewardList.append(reward)\n",
    "    \n",
    "    # Once initial placement is done use random agents till game is over\n",
    "    while True:\n",
    "        currPlayer = env.game.gameState.players[env.game.gameState.currPlayer]\n",
    "\n",
    "        agentAction = currPlayer.DoMove(env.game)\n",
    "        agentAction.ApplyAction(env.game.gameState)\n",
    "        print(len(getInputState(env.game.gameState)))\n",
    "\n",
    "        if env.game.gameState.currState == \"OVER\":\n",
    "            break\n",
    "    winner[env.game.gameState.winner] += 1\n",
    "\n",
    "\n",
    "print(winner)\n",
    "\n",
    "\n",
    "\n",
    "# plt.plot(rewardList)\n",
    "# plt.xlabel('Episode (100)')\n",
    "# plt.ylabel('Avg VP')\n",
    "# plt.show()\n",
    "\n",
    "# img = mpimg.imread('TRAINING.png')\n",
    "# plt.imshow(img)\n",
    "# plt.axis('off')  # Optional: Remove axes for a clean display\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CatanEnv import CatanSetupEnv, CatanEnv\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "\n",
    "env = CatanEnv()\n",
    "\n",
    "model = MaskablePPO(\"MlpPolicy\", env, gamma=0.4, verbose=1, tensorboard_log=\"./tensorboard_logs/\")\n",
    "model.learn(total_timesteps=500_000, tb_log_name='full_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 26, 15, 32]\n",
      "5.37\n"
     ]
    }
   ],
   "source": [
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from GameStateViewer import SaveGameStateImage\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from ModelState import getInputState\n",
    "\n",
    "##############################################################\n",
    "rewardList = []\n",
    "rewardList100 = []\n",
    "winner = [0,0,0,0]\n",
    "vpList = []\n",
    "\n",
    "for episode in range(100):\n",
    "    done = False\n",
    "    state, info = env.reset()\n",
    "\n",
    "    while done != True:\n",
    "        action_masks = get_action_masks(env)\n",
    "        action, _states = model.predict(state, action_masks=action_masks)\n",
    "        state, reward, done, _, info = env.step(action.item())\n",
    "        # print(f\"Reward: {reward+7}\")\n",
    "        rewardList.append(reward)\n",
    "    \n",
    "    winner[env.game.gameState.winner] += 1\n",
    "    vpList.append(env.players[2].victoryPoints)\n",
    "\n",
    "\n",
    "print(winner)\n",
    "print(sum(vpList)/len(vpList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieldrummond/Catan/PyCatron/TC2/Client/env/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.action_masks to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.action_masks` for environment variables or `env.get_wrapper_attr('action_masks')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2        |\n",
      "|    ep_rew_mean     | 0.16     |\n",
      "| time/              |          |\n",
      "|    fps             | 223      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.16        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013408147 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.7        |\n",
      "|    explained_variance   | -0.0833     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.469       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0526     |\n",
      "|    value_loss           | 1.29        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.2         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013213391 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.68       |\n",
      "|    explained_variance   | -0.107      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.341       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0528     |\n",
      "|    value_loss           | 1.25        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.28        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014488436 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.64       |\n",
      "|    explained_variance   | -0.197      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.18        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0578     |\n",
      "|    value_loss           | 1.3         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.2         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012620201 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.61       |\n",
      "|    explained_variance   | -0.202      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.183       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 1.27        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.72        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014291033 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.56       |\n",
      "|    explained_variance   | -0.235      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.296       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0551     |\n",
      "|    value_loss           | 1.23        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.48        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 67          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015671777 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.5        |\n",
      "|    explained_variance   | -0.219      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.233       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0599     |\n",
      "|    value_loss           | 1.31        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.48        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015994897 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.46       |\n",
      "|    explained_variance   | -0.308      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.213       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0603     |\n",
      "|    value_loss           | 1.41        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2          |\n",
      "|    ep_rew_mean          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 202        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 91         |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01663185 |\n",
      "|    clip_fraction        | 0.214      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.42      |\n",
      "|    explained_variance   | -0.262     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.287      |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0632    |\n",
      "|    value_loss           | 1.41       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2          |\n",
      "|    ep_rew_mean          | 0.96       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 204        |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 100        |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01909076 |\n",
      "|    clip_fraction        | 0.235      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.35      |\n",
      "|    explained_variance   | -0.256     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.253      |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0652    |\n",
      "|    value_loss           | 1.38       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2          |\n",
      "|    ep_rew_mean          | 0.76       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 205        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 109        |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01988315 |\n",
      "|    clip_fraction        | 0.242      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.29      |\n",
      "|    explained_variance   | -0.363     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.374      |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0679    |\n",
      "|    value_loss           | 1.54       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.88        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 120         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020572603 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.22       |\n",
      "|    explained_variance   | -0.295      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.236       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0705     |\n",
      "|    value_loss           | 1.46        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.92        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 130         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021853935 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.17       |\n",
      "|    explained_variance   | -0.357      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.312       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0708     |\n",
      "|    value_loss           | 1.54        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.56        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 139         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023442224 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.11       |\n",
      "|    explained_variance   | -0.318      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.3         |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.074      |\n",
      "|    value_loss           | 1.48        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2          |\n",
      "|    ep_rew_mean          | 0.88       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 206        |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 148        |\n",
      "|    total_timesteps      | 30720      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02460921 |\n",
      "|    clip_fraction        | 0.274      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.08      |\n",
      "|    explained_variance   | -0.421     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.361      |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0749    |\n",
      "|    value_loss           | 1.65       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.8         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 158         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024228437 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.04       |\n",
      "|    explained_variance   | -0.321      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.416       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0736     |\n",
      "|    value_loss           | 1.59        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 0.96        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 167         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026998438 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.98       |\n",
      "|    explained_variance   | -0.262      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.396       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0796     |\n",
      "|    value_loss           | 1.52        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2          |\n",
      "|    ep_rew_mean          | 1.4        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 207        |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 178        |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02882963 |\n",
      "|    clip_fraction        | 0.317      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.94      |\n",
      "|    explained_variance   | -0.25      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.238      |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.0769    |\n",
      "|    value_loss           | 1.52       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 1.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 187         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028652892 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | -0.275      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.476       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0816     |\n",
      "|    value_loss           | 1.66        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 1.04        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 196         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032286704 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.78       |\n",
      "|    explained_variance   | -0.281      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.638       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0805     |\n",
      "|    value_loss           | 1.67        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2          |\n",
      "|    ep_rew_mean          | 1.4        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 208        |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 206        |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03334237 |\n",
      "|    clip_fraction        | 0.36       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.71      |\n",
      "|    explained_variance   | -0.282     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.375      |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0846    |\n",
      "|    value_loss           | 1.63       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2         |\n",
      "|    ep_rew_mean          | 1.52      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 208       |\n",
      "|    iterations           | 22        |\n",
      "|    time_elapsed         | 215       |\n",
      "|    total_timesteps      | 45056     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0339458 |\n",
      "|    clip_fraction        | 0.354     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.65     |\n",
      "|    explained_variance   | -0.267    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.327     |\n",
      "|    n_updates            | 210       |\n",
      "|    policy_gradient_loss | -0.0844   |\n",
      "|    value_loss           | 1.67      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2          |\n",
      "|    ep_rew_mean          | 1.16       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 209        |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 224        |\n",
      "|    total_timesteps      | 47104      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03700075 |\n",
      "|    clip_fraction        | 0.366      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.58      |\n",
      "|    explained_variance   | -0.255     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.456      |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.0867    |\n",
      "|    value_loss           | 1.66       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2          |\n",
      "|    ep_rew_mean          | 1.2        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 209        |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 235        |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03652869 |\n",
      "|    clip_fraction        | 0.386      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.51      |\n",
      "|    explained_variance   | -0.236     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.418      |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.0845    |\n",
      "|    value_loss           | 1.68       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2           |\n",
      "|    ep_rew_mean          | 1.2         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 244         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038458113 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.48       |\n",
      "|    explained_variance   | -0.26       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.555       |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0821     |\n",
      "|    value_loss           | 1.71        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sb3_contrib.ppo_mask.ppo_mask.MaskablePPO at 0x171d7ff50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from DeepLearning.Env import CatanSetupRandomEnv\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "import os\n",
    "os.environ['SAVE_IMAGE'] = 'False'\n",
    "\n",
    "env = CatanSetupRandomEnv()\n",
    "\n",
    "model = MaskablePPO(\"MlpPolicy\", env, gamma=0.4, verbose=1) #, tensorboard_log=\"./tensorboard_logs/\")\n",
    "model.learn(total_timesteps=50_000) #, tb_log_name='full_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "0\n",
      "3\n",
      "0\n",
      "2\n",
      "3\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "0\n",
      "3\n",
      "1\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[52, 15, 16, 17]\n"
     ]
    }
   ],
   "source": [
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "import os\n",
    "\n",
    "# os.environ['SAVE_IMAGE'] = 'True'\n",
    "\n",
    "rewardList = []\n",
    "rewardList100 = []\n",
    "winner = [0,0,0,0]\n",
    "vpList = []\n",
    "\n",
    "for episode in range(100):\n",
    "    done = False\n",
    "    state, info = env.reset()\n",
    "\n",
    "    while done != True:\n",
    "        action_masks = get_action_masks(env)\n",
    "        action, _states = model.predict(state, action_masks=action_masks)\n",
    "        state, reward, done, _, info = env.step(action.item())\n",
    "        # print(f\"Reward: {reward+7}\")\n",
    "        rewardList.append(reward)\n",
    "\n",
    "    \n",
    "    winner[env.game.gameState.winner] += 1\n",
    "\n",
    "\n",
    "print(winner)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
