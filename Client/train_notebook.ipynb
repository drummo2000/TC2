{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepLearning.Environments.NoSetupEnv import NoSetupDenseRewardEnv\n",
    "from DeepLearning.Environments.SetupEnv import SetupRandomWithRoadsEnv\n",
    "from DeepLearning.Environments.CatanEnv import CatanEnv\n",
    "from DeepLearning.Environments.SelfPlayEnv import SelfPlayEnv, SelfPlaySetupDotTotalEnv\n",
    "from DeepLearning.PPO import MaskablePPO\n",
    "from DeepLearning.GetActionMask import getSetupWithRoadsActionMask, getActionMask\n",
    "from DeepLearning.GetObservation import getSetupRandomWithRoadsObservation, getObservation\n",
    "from Agents.AgentModel import AgentMultiModel\n",
    "import os\n",
    "\n",
    "os.environ[\"UPDATE_MODELS\"] = \"False\"\n",
    "os.environ[\"MODEL_NAME\"] = \"None\"\n",
    "\n",
    "# setupOpponentModel = MaskablePPO.load('DeepLearning/Models/SetupOnly_DotTotal_100k.zip')\n",
    "# opponentModel = MaskablePPO.load('DeepLearning/Models/SelfPlay_SetupDotTotal_7vp_2M.zip')\n",
    "# opponents = [\n",
    "#     AgentMultiModel(\"P1\", 1, setupModel=setupOpponentModel, model=opponentModel, fullSetup=False),\n",
    "#     AgentMultiModel(\"P1\", 1, setupModel=setupOpponentModel, model=opponentModel, fullSetup=False),\n",
    "#     AgentMultiModel(\"P2\", 2, setupModel=setupOpponentModel, model=opponentModel, fullSetup=False),\n",
    "#     AgentMultiModel(\"P3\", 3, setupModel=setupOpponentModel, model=opponentModel, fullSetup=False)\n",
    "# ]\n",
    "\n",
    "#env = NoSetupDenseRewardEnv(setupModel=MaskablePPO.load('DeepLearning/Models/SetupRandom_wins_1M.zip'))\n",
    "env = SelfPlayEnv()\n",
    "\n",
    "info = {\n",
    "    \"env\": \"SelfPlayEnv\",\n",
    "    \"Timesteps\": \"13M\",\n",
    "    \"Opponents\": \"3x(same model self play)\",\n",
    "    \"Rewards\": \"vp Actions\"\n",
    "}\n",
    "\n",
    "name = \"SelfPlay_7vp_13M\"\n",
    "\n",
    "model = MaskablePPO(\"MlpPolicy\", env, gamma=0.4, verbose=1, getActionMask=getActionMask, getObservation=getObservation, info=info, tensorboard_log=\"./tensorboard_logs/\")\n",
    "model.learn(total_timesteps=13_000_000, tb_log_name=name)\n",
    "# model.save(f\"DeepLearning/Models/{name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Trained model using env format, use to debug running environments \n",
    "\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from DeepLearning.Environments.NoSetupEnv import NoSetupDenseRewardEnv, NoSetupEnv\n",
    "from DeepLearning.Environments.SetupEnv import SetupRandomWithRoadsEnv\n",
    "from DeepLearning.Environments.CatanEnv import CatanEnv\n",
    "from DeepLearning.Environments.SelfPlayEnv import SelfPlayEnv, SelfPlaySetupDotTotalEnv\n",
    "import os\n",
    "from Game.CatanPlayer import PlayerStatsTracker\n",
    "from DeepLearning.PPO import MaskablePPO\n",
    "from Agents.AgentRandom2 import AgentRandom2\n",
    "from Agents.AgentMCTS import AgentMCTS\n",
    "from tabulate import tabulate\n",
    "from DeepLearning.Stats import headers\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def run():\n",
    "    # env = NoSetupDenseRewardEnv(setupModel=MaskablePPO.load('DeepLearning/Models/SetupRandom_wins_1M.zip'))\n",
    "    os.environ[\"UPDATE_MODELS\"] = \"False\"\n",
    "    os.environ[\"MODEL_NAME\"] = \"None\"\n",
    "    env = NoSetupEnv(setupModel=MaskablePPO.load('DeepLearning/Models/SetupOnly_DotTotal_100k.zip'))\n",
    "    modelName = \"model_iteration_751\"\n",
    "\n",
    "    # model.save(f\"DeepLearning/Models/{modelName}\")\n",
    "    model = MaskablePPO.load(f'DeepLearning/SelfPlayModels/{modelName}.zip')\n",
    "\n",
    "    rewardList = []\n",
    "    winner = [0,0,0,0]\n",
    "\n",
    "    stats = PlayerStatsTracker()\n",
    "    randomStats = PlayerStatsTracker()\n",
    "\n",
    "    players = [ AgentRandom2(\"P0\", 0),\n",
    "                AgentRandom2(\"P1\", 1),\n",
    "                AgentRandom2(\"P2\", 2),\n",
    "                AgentRandom2(\"P3\", 3)]\n",
    "\n",
    "    for episode in range(100):\n",
    "        done = False\n",
    "        state, info = env.reset()#players=players)\n",
    "\n",
    "        while done != True:\n",
    "            action_masks = get_action_masks(env)\n",
    "            action, _states = model.predict(state, action_masks=action_masks)\n",
    "            state, reward, done, _, info = env.step(action.item())\n",
    "            rewardList.append(reward)\n",
    "\n",
    "        winner[env.game.gameState.winner] += 1\n",
    "        \n",
    "        env.game.gameState.players[0].generatePlayerStats()\n",
    "        env.game.gameState.players[3].generatePlayerStats()\n",
    "\n",
    "        stats += env.game.gameState.players[0].stats\n",
    "        randomStats += env.game.gameState.players[3].stats\n",
    "\n",
    "    # Collect stats\n",
    "    opponentName = \"AgentRandom\"\n",
    "\n",
    "    stats.getAverages()\n",
    "    randomStats.getAverages()\n",
    "\n",
    "    agentData = stats.getList()\n",
    "    agentData.insert(0, winner[0]/sum(winner))\n",
    "    agentData.insert(0, modelName)\n",
    "    randomData = randomStats.getList()\n",
    "    randomData.insert(0, winner[3]/sum(winner))\n",
    "    randomData.insert(0, opponentName)\n",
    "\n",
    "    table = tabulate([agentData, randomData], headers=headers, tablefmt='simple')\n",
    "    print(table)\n",
    "\n",
    "    # Save to CSV\n",
    "    fileName = f'{modelName}_vs_3{opponentName}.csv'\n",
    "    # df = pd.DataFrame([agentData, randomData], columns=headers)\n",
    "    # df.to_csv(f'DeepLearning/Data/{fileName}', index=False)\n",
    "\n",
    "\n",
    "    print(\"\\n\\nWinnings: \", winner)\n",
    "\n",
    "run()\n",
    "\n",
    "# Brick, ore, wool, wheat, wood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentName      WinRate    MarginError    numTurns    victoryPoints  finalTradeRates                       numRoadsBuilt    devCardsBought  usedDevCards                        settlementsBuilt    citiesBuilt    devCardVP    largestArmy    longestRoad  resourcesReceived                          totalResourcesReceivedPerTurn    totalResourcesDiscarded    totalResourcesStolen  resourcesFromDevCard               totalResourcesFromDevCard  resourcesFromBankTrade               totalResourcesFromBankTrade  finalResourceProduction               setupResourceProduction              totalSetupResourceProduction  setupTradeRates                       setupResourceDiversity\n",
      "-----------  ---------  -------------  ----------  ---------------  ----------------------------------  ---------------  ----------------  --------------------------------  ------------------  -------------  -----------  -------------  -------------  ---------------------------------------  -------------------------------  -------------------------  ----------------------  -------------------------------  ---------------------------  ---------------------------------  -----------------------------  ------------------------------------  ---------------------------------  ------------------------------  ----------------------------------  ------------------------\n",
      "Player0          0.486           4.38      39.576            8.062  [3.608, 3.68, 3.64, 3.622, 3.646]             9.738             5.19   [2.866, 0.314, 0.414, 0.4, 0.0]                1.756          1.838        1.014          0.502          0.248  [19.36, 26.608, 23.808, 34.158, 30.15]                             3.388                     19.66                    7.868  [0.068, 1.764, 0.0, 0.008, 0.0]                         1.84  [3.87, 5.63, 3.826, 6.934, 4.246]                         24.506  [6.636, 8.938, 8.292, 11.766, 9.972]  [3.224, 3.976, 3.9, 5.266, 4.508]                              21  [3.99, 3.998, 3.99, 3.998, 3.998]                      3.816\n",
      "Player1          0.478           4.38      39.334            7.85   [3.518, 3.526, 3.558, 3.554, 3.53]           12.256             4.338  [2.438, 0.25, 0.298, 0.332, 0.0]               2.23           1.376        0.836          0.362          0.614  [24.73, 27.226, 18.092, 30.348, 28.404]                            3.275                     17.888                   7.55   [0.298, 0.954, 0.298, 0.0, 0.0]                         1.55  [9.564, 3.388, 5.12, 1.24, 4.278]                         23.59   [7.754, 8.914, 7.026, 10.468, 9.158]  [4.11, 4.412, 2.86, 5.002, 4.834]                              21  [3.986, 3.998, 3.998, 3.994, 3.99]                     3.868\n",
      "\n",
      "\n",
      "Winnings:  [243, 239, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Running Agent simulations\n",
    "\"\"\"\n",
    "from Agents.AgentRandom2 import AgentRandom2\n",
    "from Agents.AgentMCTS import AgentMCTS\n",
    "from Agents.AgentUCT import AgentUCT\n",
    "from Agents.AgentModel import AgentMultiModel, AgentModel\n",
    "from Game.CatanGame import *\n",
    "from CatanSimulator import CreateGame\n",
    "from DeepLearning.PPO import MaskablePPO\n",
    "from Game.CatanPlayer import PlayerStatsTracker\n",
    "from tabulate import tabulate\n",
    "from DeepLearning.Stats import headers\n",
    "import dill as pickle\n",
    "import pandas as pd\n",
    "from CatanData.GameStateViewer import SaveGameStateImage, DisplayImage\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "winner = [0,0,0,0]\n",
    "player0Stats = PlayerStatsTracker()\n",
    "player1Stats = PlayerStatsTracker()\n",
    "\n",
    "players = [ AgentMultiModel(\"P0\", 0, setupModel=MaskablePPO.load('DeepLearning/Models/SetupRoadsSelfPlay_v_SelfPlay_SetupDotTotal_wins_1M.zip'), fullSetup=True, model=MaskablePPO.load('DeepLearning/Models/SelfPlay/SelfPlay_SetupDotTotal_7vp_2M.zip')),\n",
    "            AgentMultiModel(\"P1\", 1, setupModel=MaskablePPO.load('DeepLearning/Models/Setup/SetupRandom_wins_1M.zip'), fullSetup=False, model=MaskablePPO.load('DeepLearning/Models/NoSetup/NoSetupDenseRewardEnv-10M.zip')),\n",
    "            AgentRandom2(\"P2\", 2),\n",
    "            AgentRandom2(\"P3\", 3)]\n",
    "\n",
    "COLLECT_STATS = True\n",
    "\n",
    "for episode in range(500):\n",
    "    game = CreateGame(players)\n",
    "    game = pickle.loads(pickle.dumps(game, -1))\n",
    "    while True:\n",
    "        currPlayer = game.gameState.players[game.gameState.currPlayer]\n",
    "\n",
    "        agentAction = currPlayer.DoMove(game)\n",
    "        agentAction.ApplyAction(game.gameState)\n",
    "\n",
    "        if game.gameState.currState == \"OVER\":\n",
    "            # DisplayImage(game.gameState)\n",
    "            break\n",
    "    winner[game.gameState.winner] += 1\n",
    "\n",
    "    # Stats\n",
    "    if COLLECT_STATS:\n",
    "        game.gameState.players[0].generatePlayerStats()\n",
    "        game.gameState.players[1].generatePlayerStats()\n",
    "\n",
    "        player0Stats += game.gameState.players[0].stats\n",
    "        player1Stats += game.gameState.players[1].stats\n",
    "\n",
    "# Collect stats\n",
    "if COLLECT_STATS:\n",
    "    player0Stats.getAverages()\n",
    "    player1Stats.getAverages()\n",
    "\n",
    "    player0Data = player0Stats.getList()\n",
    "    player1Data = player1Stats.getList()\n",
    "\n",
    "    p_hat0 = winner[0] / sum(winner)\n",
    "    p_hat1 = winner[1] / sum(winner)\n",
    "    margin_error0 = round(100*(1.96 * math.sqrt((p_hat0 * (1 - p_hat0)) / sum(winner))), 2)\n",
    "    margin_error1 = round(100*(1.96 * math.sqrt((p_hat1 * (1 - p_hat1)) / sum(winner))), 2)\n",
    "    player0Data.insert(0, margin_error0)\n",
    "    player1Data.insert(0, margin_error1)\n",
    "    player0Data.insert(0, winner[0]/sum(winner))\n",
    "    player1Data.insert(0, winner[1]/sum(winner))\n",
    "    player0Data.insert(0, \"Player0\")\n",
    "    player1Data.insert(0, \"Player1\")\n",
    "\n",
    "    table = tabulate([player0Data, player1Data], headers=headers, tablefmt='simple')\n",
    "    print(table)\n",
    "\n",
    "\n",
    "print(\"\\n\\nWinnings: \", winner)\n",
    "\n",
    "\n",
    "# Save to csv\n",
    "# fileName = f'SelfPlay_SetupDotTotal_7vp_2M_vs_NoSetupDenseRewardEnv-10M.csv'\n",
    "# df = pd.DataFrame([player0Data, player1Data], columns=headers)\n",
    "# df.to_csv(f'DeepLearning/Data/{fileName}', index=False)\n",
    "\n",
    "# Brick, ore, wool, wheat, wood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function getActionMask at 0x177b0e660>\n",
      "<function getObservation at 0x177b0df80>\n"
     ]
    }
   ],
   "source": [
    "# Update models functions\n",
    "from DeepLearning.PPO import MaskablePPO\n",
    "from DeepLearning.GetObservation import getObservation\n",
    "\n",
    "model=MaskablePPO.load('DeepLearning/Models/Full_vp_100k.zip')\n",
    "print(model.getActionMask)\n",
    "print(model.getObservation)\n",
    "# model.getObservation = getObservation\n",
    "# model.save('DeepLearning/Models/Full_vp_100k.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
