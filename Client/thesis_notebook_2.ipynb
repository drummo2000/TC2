{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepLearning.PPO import MaskablePPO\n",
    "from DeepLearning.Thesis.Environments.HyperparameterTesting import HyperParamVpEndEnv\n",
    "from DeepLearning.GetActionMask import getActionMask\n",
    "from DeepLearning.Thesis.Observations.get_observation import getObservation, lowerBound, upperBound\n",
    "import os\n",
    "\n",
    "env = HyperParamVpEndEnv(lowerBounds=lowerBound, upperBounds=upperBound, getObservationFunction=getObservation, getActionMaskFunction=getActionMask)\n",
    "actionMask = getActionMask\n",
    "observation = getObservation\n",
    "\n",
    "netArchDict = dict(pi=[128, 128, 128], vf=[128, 128, 128])\n",
    "gamma = 0.99\n",
    "\n",
    "\n",
    "saveName = \"NumSteps_8192\"\n",
    "savePath = f\"DeepLearning/Thesis/Hyperparameters/Models/{saveName}\"\n",
    "\n",
    "model = MaskablePPO(\"MlpPolicy\", env, verbose=1, policy_kwargs=dict(net_arch=netArchDict), gamma=gamma, n_steps=8192, getActionMask=actionMask, getObservation=observation, savePath=savePath, tensorboard_log=\"./tensorboard_logs_thesis/\")\n",
    "model.learn(total_timesteps=1_000_000, tb_log_name=saveName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Running Agent simulations\n",
    "\"\"\"\n",
    "from Agents.AgentRandom2 import AgentRandom2\n",
    "from Agents.AgentMCTS import AgentMCTS\n",
    "from Agents.AgentUCT import AgentUCT\n",
    "from Agents.AgentModel import AgentMultiModel, AgentModel\n",
    "from Game.CatanGame import *\n",
    "from CatanSimulator import CreateGame\n",
    "from DeepLearning.PPO import MaskablePPO\n",
    "from Game.CatanPlayer import PlayerStatsTracker\n",
    "from tabulate import tabulate\n",
    "from DeepLearning.Stats import headers\n",
    "import dill as pickle\n",
    "from CatanData.GameStateViewer import SaveGameStateImage, DisplayImage\n",
    "import math\n",
    "from DeepLearning.Environments.CatanEnv import CatanEnv, SelfPlayDistribution, CatanTradingEnv, SelfPlayDistTradingEnv\n",
    "\n",
    "from DeepLearning.Thesis.Observations.nodes_v_hexes import getNodeInHexObs, nodeInHexLowerBound, nodeInHexUpperBound\n",
    "\n",
    "winner = [0,0,0,0]\n",
    "player0Stats = PlayerStatsTracker()\n",
    "Player0LosingStats = PlayerStatsTracker()\n",
    "player1Stats = PlayerStatsTracker()\n",
    "\n",
    "\n",
    "testModel = MaskablePPO.load(\"DeepLearning/Thesis/Hyperparameters/Models/NumSteps_8192/Final.zip\")\n",
    "# testModel2 = MaskablePPO.load(\"DeepLearning/Thesis/Hyperparameters/Models/NetworkArch_128_128/Final.zip\")\n",
    "# testModel3 = MaskablePPO.load(\"DeepLearning/Thesis/Hyperparameters/Models/NetworkArch_256_256/Final.zip\")\n",
    "# testModel4 = MaskablePPO.load(\"DeepLearning/Thesis/Hyperparameters/Models/NetworkArch_512_512/Final.zip\")\n",
    "\n",
    "players = [ AgentModel(\"P0\", 0, recordStats=True, playerTrading=False, model=testModel),\n",
    "            # AgentModel(\"P1\", 1, recordStats=True, playerTrading=False, model=testModel2),\n",
    "            AgentRandom2(\"P1\", 1, recordStats=True, playerTrading=False),\n",
    "            AgentRandom2(\"P2\", 2, recordStats=True, playerTrading=False),\n",
    "            AgentRandom2(\"P3\", 3, recordStats=True, playerTrading=False),]\n",
    "\n",
    "\n",
    "COLLECT_STATS = True\n",
    "for episode in range(2000):\n",
    "    game = CreateGame(players)\n",
    "    game = pickle.loads(pickle.dumps(game, -1))\n",
    "    numTurns = 0\n",
    "    while True:\n",
    "        currPlayer = game.gameState.players[game.gameState.currPlayer]\n",
    "\n",
    "        agentAction = currPlayer.DoMove(game)\n",
    "        agentAction.ApplyAction(game.gameState)\n",
    "\n",
    "        if currPlayer.seatNumber == 0 and agentAction.type == 'EndTurn':\n",
    "        #     DisplayImage(game.gameState, agentAction)\n",
    "        #     time.sleep(1)\n",
    "            numTurns += 1\n",
    "\n",
    "        if game.gameState.currState == \"OVER\":\n",
    "            # DisplayImage(game.gameState, agentAction)\n",
    "            break\n",
    "    \n",
    "    # print(\"Winner: \", game.gameState.winner)\n",
    "    winner[game.gameState.winner] += 1\n",
    "    lost = game.gameState.winner != 0\n",
    "\n",
    "    # Stats\n",
    "    if COLLECT_STATS:\n",
    "        game.gameState.players[0].generatePlayerStats()\n",
    "        game.gameState.players[1].generatePlayerStats()\n",
    "\n",
    "        player0Stats += game.gameState.players[0].stats\n",
    "        player1Stats += game.gameState.players[1].stats\n",
    "        if lost:\n",
    "            Player0LosingStats += game.gameState.players[0].stats\n",
    "\n",
    "# Collect stats\n",
    "if COLLECT_STATS:\n",
    "    player0Stats.getAverages()\n",
    "    Player0LosingStats.getAverages()\n",
    "    player1Stats.getAverages()\n",
    "    player0Data = player0Stats.getList()\n",
    "    player0LosingData = Player0LosingStats.getList()\n",
    "    player1Data = player1Stats.getList()\n",
    "\n",
    "    p_hat0 = winner[0] / sum(winner)\n",
    "    p_hat1 = winner[1] / sum(winner)\n",
    "    margin_error0 = round(100*(1.96 * math.sqrt((p_hat0 * (1 - p_hat0)) / sum(winner))), 2)\n",
    "    margin_error1 = round(100*(1.96 * math.sqrt((p_hat1 * (1 - p_hat1)) / sum(winner))), 2)\n",
    "    player0Data.insert(0, margin_error0)\n",
    "    player0LosingData.insert(0, -1)\n",
    "    player1Data.insert(0, margin_error1)\n",
    "    player0Data.insert(0, winner[0]/sum(winner))\n",
    "    player0LosingData.insert(0, -1)\n",
    "    player1Data.insert(0, winner[1]/sum(winner))\n",
    "    player0Data.insert(0, \"Player0\")\n",
    "    player0LosingData.insert(0, \"Player0LossesStats\")\n",
    "    player1Data.insert(0, \"Player1\")\n",
    "\n",
    "    table = tabulate([player0Data, player0LosingData, player1Data], headers=headers, tablefmt='simple')\n",
    "    print(table)\n",
    "\n",
    "print(f\"\\nNum turns: {numTurns}\")\n",
    "\n",
    "print(\"\\n\\nWinnings: \", winner)\n",
    "\n",
    "\n",
    "# Brick, ore, wool, wheat, wood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Save to csv\n",
    "fileName = f'NumSteps_8192_v_3Random.csv'\n",
    "df = pd.DataFrame([player0Data, player0LosingData, player1Data], columns=headers)\n",
    "df.to_csv(f'DeepLearning/Thesis/Hyperparameters/Data/{fileName}', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
