{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieldrummond/Catan/PyCatron/TC2/Client/env/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.action_masks to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.action_masks` for environment variables or `env.get_wrapper_attr('action_masks')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.5     |\n",
      "|    ep_rew_mean     | 6.67     |\n",
      "| time/              |          |\n",
      "|    fps             | 1334     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 94.6        |\n",
      "|    ep_rew_mean          | 6.05        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 937         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016774988 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.92       |\n",
      "|    explained_variance   | -0.00417    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.224       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.04       |\n",
      "|    value_loss           | 0.874       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 95.6        |\n",
      "|    ep_rew_mean          | 6.16        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 922         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021471696 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.88       |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0603     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0455     |\n",
      "|    value_loss           | 0.109       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 93.3        |\n",
      "|    ep_rew_mean          | 6.13        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 817         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018786397 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.86       |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0755     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0371     |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 93.9        |\n",
      "|    ep_rew_mean          | 6.08        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 794         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019541953 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.86       |\n",
      "|    explained_variance   | 0.781       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00715    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0377     |\n",
      "|    value_loss           | 0.0992      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieldrummond/Catan/PyCatron/TC2/Client/env/lib/python3.11/site-packages/stable_baselines3/common/save_util.py:283: UserWarning: Path 'DeepLearning/Thesis/Observations/Models/final_observation' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DeepLearning.PPO.MaskablePPO at 0x2949af5d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from DeepLearning.PPO import MaskablePPO\n",
    "from DeepLearning.Thesis.Environments.ObservationTesting import ObsTestingEnv\n",
    "from DeepLearning.GetActionMask import getActionMask\n",
    "from DeepLearning.Thesis.Observations.get_observation import getObservation, lowerBound, upperBound\n",
    "import os\n",
    "\n",
    "env = ObsTestingEnv(lowerBounds=lowerBound, upperBounds=upperBound, getObservationFunction=getObservation)\n",
    "actionMask = getActionMask\n",
    "observation = getObservation\n",
    "\n",
    "saveName = \"final_observation\"\n",
    "savePath = f\"DeepLearning/Thesis/Observations/Models/{saveName}\"\n",
    "\n",
    "model = MaskablePPO(\"MlpPolicy\", env, verbose=1, getActionMask=actionMask, getObservation=observation, savePath=savePath) #, tensorboard_log=\"./tensorboard_logs_thesis/\")\n",
    "model.learn(total_timesteps=10_000)# , tb_log_name=saveName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentName             WinRate    MarginError    numTurns    victoryPoints    numRoadsBuilt    devCardsBought  usedDevCards                 settlementsBuilt    citiesBuilt    devCardVP    largestArmy    longestRoad  resourcesReceived                 totalResourcesReceivedPerTurn    totalResourcesDiscarded    totalResourcesStolen  resourcesFromDevCard         totalResourcesFromDevCard  resourcesFromBankTrade        finalResourceProduction      finalTradeRates            setupResourceProduction      totalSetupResourceProduction  setupTradeRates              setupResourceDiversity    turnsForFirstSettlement    noSettlementsBuilt    turnsForFirstCity    noCitysBuilt    numRoadsFor1stSettlement    totalResourcesFromBankTrade    goodSettlementBankTrades    badSettlementBankTrades    goodCityBankTrades    badCityBankTrades    goodRoadBankTrades    badRoadBankTrades    goodDevCardBankTrades    badDevCardBankTrades    neutralBankTrades    acceptedTrades    rejectedTrades    goodSettlementAcceptedTrades    badSettlementAcceptedTrades    goodCityAcceptedTrades    badCityAcceptedTrades    goodRoadAcceptedTrades    badRoadAcceptedTrades    goodDevCardAcceptedTrades    badDevCardAcceptedTrades    neutralAcceptedTrades    totalPlayerTrades    acceptedPlayerTrades    goodSettlementPlayerTrades    badSettlementPlayerTrades    goodCityPlayerTrades    badCityPlayerTrades    goodRoadPlayerTrades    badRoadPlayerTrades    goodDevCardPlayerTrades    badDevCardPlayerTrades    neutralPlayerTrades\n",
      "------------------  ---------  -------------  ----------  ---------------  ---------------  ----------------  -------------------------  ------------------  -------------  -----------  -------------  -------------  ------------------------------  -------------------------------  -------------------------  ----------------------  -------------------------  ---------------------------  ----------------------------  ---------------------------  -------------------------  -------------------------  ------------------------------  -------------------------  ------------------------  -------------------------  --------------------  -------------------  --------------  --------------------------  -----------------------------  --------------------------  -------------------------  --------------------  -------------------  --------------------  -------------------  -----------------------  ----------------------  -------------------  ----------------  ----------------  ------------------------------  -----------------------------  ------------------------  -----------------------  ------------------------  -----------------------  ---------------------------  --------------------------  -----------------------  -------------------  ----------------------  ----------------------------  ---------------------------  ----------------------  ---------------------  ----------------------  ---------------------  -------------------------  ------------------------  ---------------------\n",
      "Player0                     0              0         102                4                7                 5  [4.0, 0.0, 0.0, 0.0, 0.0]                   0              1            1              0              0  [0.0, 12.0, 0.0, 105.0, 32.0]                             1.461                         27                      11  [0.0, 0.0, 0.0, 0.0, 0.0]                            0  [5.0, 8.0, 7.0, 0.0, 9.0]     [0.0, 1.0, 0.0, 8.0, 3.0]    [4.0, 4.0, 4.0, 4.0, 4.0]  [0.0, 1.0, 0.0, 8.0, 3.0]                              12  [4.0, 4.0, 4.0, 4.0, 4.0]                         3                         -1                     1                   96               0                          -1                             29                           0                          0                     0                    0                     2                    0                        5                       3                   15                 0                 0                               0                              0                         0                        0                         0                        0                            0                           0                        0                    0                       0                             0                            0                       0                      0                       0                      0                          0                         0                      0\n",
      "Player0LossesStats         -1             -1         102                4                7                 5  [4.0, 0.0, 0.0, 0.0, 0.0]                   0              1            1              0              0  [0.0, 12.0, 0.0, 105.0, 32.0]                             1.461                         27                      11  [0.0, 0.0, 0.0, 0.0, 0.0]                            0  [5.0, 8.0, 7.0, 0.0, 9.0]     [0.0, 1.0, 0.0, 8.0, 3.0]    [4.0, 4.0, 4.0, 4.0, 4.0]  [0.0, 1.0, 0.0, 8.0, 3.0]                              12  [4.0, 4.0, 4.0, 4.0, 4.0]                         3                         -1                     1                   96               0                          -1                             29                           0                          0                     0                    0                     2                    0                        5                       3                   15                 0                 0                               0                              0                         0                        0                         0                        0                            0                           0                        0                    0                       0                             0                            0                       0                      0                       0                      0                          0                         0                      0\n",
      "Player1                     1              0         102               10               15                10  [3.0, 1.0, 2.0, 1.0, 0.0]                   2              3            3              0              0  [13.0, 73.0, 49.0, 60.0, 34.0]                            2.245                         17                      20  [3.0, 1.0, 0.0, 0.0, 0.0]                            4  [8.0, 9.0, 15.0, 17.0, 14.0]  [6.0, 13.0, 19.0, 8.0, 8.0]  [3.0, 3.0, 3.0, 3.0, 3.0]  [1.0, 5.0, 3.0, 4.0, 2.0]                              15  [3.0, 3.0, 3.0, 3.0, 3.0]                         5                         94                     0                   51               0                          13                             63                           4                          0                     0                    9                    10                    0                        2                      10                   37                 0                 0                               0                              0                         0                        0                         0                        0                            0                           0                        0                    0                       0                             0                            0                       0                      0                       0                      0                          0                         0                      0\n",
      "\n",
      "Num turns: 102\n",
      "\n",
      "\n",
      "Winnings:  [0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Running Agent simulations\n",
    "\"\"\"\n",
    "from Agents.AgentRandom2 import AgentRandom2\n",
    "from Agents.AgentMCTS import AgentMCTS\n",
    "from Agents.AgentUCT import AgentUCT\n",
    "from Agents.AgentModel import AgentMultiModel, AgentModel\n",
    "from Game.CatanGame import *\n",
    "from CatanSimulator import CreateGame\n",
    "from DeepLearning.PPO import MaskablePPO\n",
    "from Game.CatanPlayer import PlayerStatsTracker\n",
    "from tabulate import tabulate\n",
    "from DeepLearning.Stats import headers\n",
    "import dill as pickle\n",
    "from CatanData.GameStateViewer import SaveGameStateImage, DisplayImage\n",
    "import math\n",
    "from DeepLearning.Environments.CatanEnv import CatanEnv, SelfPlayDistribution, CatanTradingEnv, SelfPlayDistTradingEnv\n",
    "\n",
    "from DeepLearning.Thesis.Observations.nodes_v_hexes import getNodeInHexObs, nodeInHexLowerBound, nodeInHexUpperBound\n",
    "\n",
    "winner = [0,0,0,0]\n",
    "player0Stats = PlayerStatsTracker()\n",
    "Player0LosingStats = PlayerStatsTracker()\n",
    "player1Stats = PlayerStatsTracker()\n",
    "\n",
    "# bestModel = MaskablePPO.load(\"DeepLearning/Models/Full/Rewards_SetupVpActionDenseTrades_8M.zip\")\n",
    "\n",
    "testModel = MaskablePPO.load(\"DeepLearning/Thesis/Observations/Models/final_observation/model_10240.zip\")\n",
    "# testModel2 = MaskablePPO.load(\"DeepLearning/Thesis/Observations/Models/observation_hex/Final.zip\")\n",
    "\n",
    "players = [ AgentModel(\"P0\", 0, recordStats=True, playerTrading=False, model=testModel),\n",
    "            # AgentModel(\"P1\", 1, recordStats=True, playerTrading=False, model=testModel2),\n",
    "            AgentRandom2(\"P1\", 1, recordStats=True, playerTrading=False),\n",
    "            AgentRandom2(\"P2\", 2, recordStats=True, playerTrading=False),\n",
    "            AgentRandom2(\"P3\", 3, recordStats=True, playerTrading=False),]\n",
    "\n",
    "\n",
    "COLLECT_STATS = True\n",
    "for episode in range(1):\n",
    "    game = CreateGame(players)\n",
    "    game = pickle.loads(pickle.dumps(game, -1))\n",
    "    numTurns = 0\n",
    "    while True:\n",
    "        currPlayer = game.gameState.players[game.gameState.currPlayer]\n",
    "\n",
    "        agentAction = currPlayer.DoMove(game)\n",
    "        agentAction.ApplyAction(game.gameState)\n",
    "\n",
    "        if currPlayer.seatNumber == 0 and agentAction.type == 'EndTurn':\n",
    "        #     DisplayImage(game.gameState, agentAction)\n",
    "        #     time.sleep(1)\n",
    "            numTurns += 1\n",
    "\n",
    "        if game.gameState.currState == \"OVER\":\n",
    "            # DisplayImage(game.gameState, agentAction)\n",
    "            break\n",
    "    \n",
    "    # print(\"Winner: \", game.gameState.winner)\n",
    "    winner[game.gameState.winner] += 1\n",
    "    lost = game.gameState.winner != 0\n",
    "\n",
    "    # Stats\n",
    "    if COLLECT_STATS:\n",
    "        game.gameState.players[0].generatePlayerStats()\n",
    "        game.gameState.players[1].generatePlayerStats()\n",
    "\n",
    "        player0Stats += game.gameState.players[0].stats\n",
    "        player1Stats += game.gameState.players[1].stats\n",
    "        if lost:\n",
    "            Player0LosingStats += game.gameState.players[0].stats\n",
    "\n",
    "# Collect stats\n",
    "if COLLECT_STATS:\n",
    "    player0Stats.getAverages()\n",
    "    Player0LosingStats.getAverages()\n",
    "    player1Stats.getAverages()\n",
    "    player0Data = player0Stats.getList()\n",
    "    player0LosingData = Player0LosingStats.getList()\n",
    "    player1Data = player1Stats.getList()\n",
    "\n",
    "    p_hat0 = winner[0] / sum(winner)\n",
    "    p_hat1 = winner[1] / sum(winner)\n",
    "    margin_error0 = round(100*(1.96 * math.sqrt((p_hat0 * (1 - p_hat0)) / sum(winner))), 2)\n",
    "    margin_error1 = round(100*(1.96 * math.sqrt((p_hat1 * (1 - p_hat1)) / sum(winner))), 2)\n",
    "    player0Data.insert(0, margin_error0)\n",
    "    player0LosingData.insert(0, -1)\n",
    "    player1Data.insert(0, margin_error1)\n",
    "    player0Data.insert(0, winner[0]/sum(winner))\n",
    "    player0LosingData.insert(0, -1)\n",
    "    player1Data.insert(0, winner[1]/sum(winner))\n",
    "    player0Data.insert(0, \"Player0\")\n",
    "    player0LosingData.insert(0, \"Player0LossesStats\")\n",
    "    player1Data.insert(0, \"Player1\")\n",
    "\n",
    "    table = tabulate([player0Data, player0LosingData, player1Data], headers=headers, tablefmt='simple')\n",
    "    print(table)\n",
    "\n",
    "print(f\"\\nNum turns: {numTurns}\")\n",
    "\n",
    "print(\"\\n\\nWinnings: \", winner)\n",
    "\n",
    "\n",
    "# Brick, ore, wool, wheat, wood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Save to csv\n",
    "# fileName = f'singeOpponentObs_v_defaultOpponentObs.csv'\n",
    "# df = pd.DataFrame([player0Data, player0LosingData, player1Data], columns=headers)\n",
    "# df.to_csv(f'DeepLearning/Thesis/Observations/Data/{fileName}', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
