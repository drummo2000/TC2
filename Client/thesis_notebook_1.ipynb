{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieldrummond/Catan/PyCatron/TC2/Client/env/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.action_masks to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.action_masks` for environment variables or `env.get_wrapper_attr('action_masks')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 54.3     |\n",
      "|    ep_rew_mean     | -33.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 1216     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 56.3        |\n",
      "|    ep_rew_mean          | -28.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 823         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012007497 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.91       |\n",
      "|    explained_variance   | -0.0151     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 73.4        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0388     |\n",
      "|    value_loss           | 255         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieldrummond/Catan/PyCatron/TC2/Client/env/lib/python3.11/site-packages/stable_baselines3/common/save_util.py:283: UserWarning: Path 'DeepLearning/Thesis/6.DenseRewards/Models/TurnLimitDenseLRoad' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 58          |\n",
      "|    ep_rew_mean          | -17.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 715         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012821812 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.85       |\n",
      "|    explained_variance   | 0.419       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.7        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0372     |\n",
      "|    value_loss           | 120         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# model = MaskablePPO.load(\"DeepLearning/Thesis/DenseRewards/Models/Reward_build_trade/Reward_build_trade_2M.zip\", env)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39msavePath \u001b[38;5;241m=\u001b[39m savePath\n\u001b[0;32m---> 23\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m14_000_000\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#, tb_log_name=saveName)\u001b[39;00m\n",
      "File \u001b[0;32m~/Catan/PyCatron/TC2/Client/DeepLearning/PPO.py:585\u001b[0m, in \u001b[0;36mMaskablePPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, use_masking, progress_bar)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mrecord(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime/total_timesteps\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps, exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdump(step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps)\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Catan/PyCatron/TC2/Client/DeepLearning/PPO.py:502\u001b[0m, in \u001b[0;36mMaskablePPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     th\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n\u001b[0;32m--> 502\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Catan/PyCatron/TC2/Client/env/lib/python3.11/site-packages/torch/optim/optimizer.py:361\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m cast(Optimizer, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    360\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile_name\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;66;03m# call optimizer step pre hooks\u001b[39;00m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pre_hook \u001b[38;5;129;01min\u001b[39;00m chain(_global_optimizer_pre_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_pre_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    364\u001b[0m         result \u001b[38;5;241m=\u001b[39m pre_hook(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n",
      "File \u001b[0;32m~/Catan/PyCatron/TC2/Client/env/lib/python3.11/site-packages/torch/autograd/profiler.py:627\u001b[0m, in \u001b[0;36mrecord_function.__init__\u001b[0;34m(self, name, args)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks_on_exit: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;66;03m# TODO: TorchScript ignores standard type annotation here\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;66;03m# self.record: Optional[\"torch.classes.profiler._RecordFunction\"] = None\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(\n\u001b[0;32m--> 627\u001b[0m     \u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch.classes.profiler._RecordFunction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    628\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7/Frameworks/Python.framework/Versions/3.11/lib/python3.11/typing.py:359\u001b[0m, in \u001b[0;36m_tp_cache.<locals>.decorator.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    356\u001b[0m cached \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mlru_cache(typed\u001b[38;5;241m=\u001b[39mtyped)(func)\n\u001b[1;32m    357\u001b[0m _cleanups\u001b[38;5;241m.\u001b[39mappend(cached\u001b[38;5;241m.\u001b[39mcache_clear)\n\u001b[0;32m--> 359\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cached(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from DeepLearning.PPO import MaskablePPO\n",
    "from DeepLearning.Thesis.Environments.TurnLimitDense import TurnLimitDenseLRoad\n",
    "from DeepLearning.GetActionMask import getActionMask\n",
    "from DeepLearning.Thesis.get_observation import getObservation\n",
    "import os\n",
    "\n",
    "os.environ[\"TURN_LIMIT\"] = \"50\"\n",
    "\n",
    "env = TurnLimitDenseLRoad()\n",
    "actionMask = getActionMask\n",
    "observation = getObservation\n",
    "\n",
    "netArchDict = dict(pi=[128, 128, 128], vf=[128, 128, 128])\n",
    "gamma = 0.99\n",
    "n_steps = 4096\n",
    "\n",
    "saveName = \"TurnLimitDenseLRoad\"\n",
    "savePath = f\"DeepLearning/Thesis/6.DenseRewards/Models/{saveName}\"\n",
    "\n",
    "model = MaskablePPO(\"MlpPolicy\", env, verbose=1, policy_kwargs=dict(net_arch=netArchDict), gamma=gamma, n_steps=n_steps, getActionMask=actionMask, getObservation=observation, savePath=savePath, tensorboard_log=\"./tensorboard_logs_thesis/\")\n",
    "# model = MaskablePPO.load(\"DeepLearning/Thesis/DenseRewards/Models/Reward_build_trade/Reward_build_trade_2M.zip\", env)\n",
    "model.savePath = savePath\n",
    "model.learn(total_timesteps=14_000_000, tb_log_name=saveName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentName             WinRate    MarginError    numTurns    victoryPoints    numRoadsBuilt    devCardsBought  usedDevCards                         settlementsBuilt    citiesBuilt    devCardVP    largestArmy    longestRoad  resourcesReceived                           totalResourcesReceivedPerTurn    totalResourcesDiscarded    totalResourcesStolen  resourcesFromDevCard                   totalResourcesFromDevCard  resourcesFromBankTrade               finalResourceProduction                 finalTradeRates                                                                                       setupResourceProduction                totalSetupResourceProduction  setupTradeRates                       setupResourceDiversity    turnsForFirstSettlement    noSettlementsBuilt    turnsForFirstCity    noCitysBuilt    numRoadsFor1stSettlement    totalResourcesFromBankTrade    goodSettlementBankTrades    badSettlementBankTrades    goodCityBankTrades    badCityBankTrades    goodRoadBankTrades    badRoadBankTrades    goodDevCardBankTrades    badDevCardBankTrades    neutralBankTrades    acceptedTrades    rejectedTrades    goodSettlementAcceptedTrades    badSettlementAcceptedTrades    goodCityAcceptedTrades    badCityAcceptedTrades    goodRoadAcceptedTrades    badRoadAcceptedTrades    goodDevCardAcceptedTrades    badDevCardAcceptedTrades    neutralAcceptedTrades    totalPlayerTrades    acceptedPlayerTrades    goodSettlementPlayerTrades    badSettlementPlayerTrades    goodCityPlayerTrades    badCityPlayerTrades    goodRoadPlayerTrades    badRoadPlayerTrades    goodDevCardPlayerTrades    badDevCardPlayerTrades    neutralPlayerTrades\n",
      "------------------  ---------  -------------  ----------  ---------------  ---------------  ----------------  ---------------------------------  ------------------  -------------  -----------  -------------  -------------  ----------------------------------------  -------------------------------  -------------------------  ----------------------  -----------------------------------  ---------------------------  -----------------------------------  --------------------------------------  ----------------------------------------------------------------------------------------------------  -----------------------------------  ------------------------------  ----------------------------------  ------------------------  -------------------------  --------------------  -------------------  --------------  --------------------------  -----------------------------  --------------------------  -------------------------  --------------------  -------------------  --------------------  -------------------  -----------------------  ----------------------  -------------------  ----------------  ----------------  ------------------------------  -----------------------------  ------------------------  -----------------------  ------------------------  -----------------------  ---------------------------  --------------------------  -----------------------  -------------------  ----------------------  ----------------------------  ---------------------------  ----------------------  ---------------------  ----------------------  ---------------------  -------------------------  ------------------------  ---------------------\n",
      "Player0                0.9495           0.96     34.53           10.0465           11.1385           7.1805   [3.837, 0.475, 0.543, 0.546, 0.0]             2.1765        1.601         1.458         0.662          0.74      [17.311, 18.944, 23.966, 24.428, 24.223]                            3.153                    13.0685                  6.8205  [0.572, 1.148, 0.455, 0.164, 0.192]                        2.531  [3.637, 3.789, 1.56, 3.341, 1.819]   [8.609, 9.364, 11.225, 12.203, 11.575]  [3.935, 3.93, 3.931, 3.939, 3.941]                                                                    [3.252, 3.37, 4.39, 4.454, 4.38]                             19.846  [4.0, 4.0, 4.0, 4.0, 4.0]                            3.788                     17.2282                0.078               15.7716          0.206                       4.48717                         14.146                    0.707                    0.01                   0.2425              0.006                    2.8505              0.193                     1.848                   0.3455                8.148                  0                 0                               0                              0                         0                        0                         0                        0                            0                           0                        0                    0                       0                             0                            0                       0                      0                       0                      0                          0                         0                      0\n",
      "Player0LossesStats    -1               -1        51.7426          6.36634          11.1089           6.43564  [3.792, 0.356, 0.515, 0.416, 0.0]             1.14851       0.841584      1.12871       0.316832       0.237624  [16.396, 25.921, 37.446, 27.673, 27.941]                            2.616                    22.1485                 10.297   [0.574, 0.683, 0.356, 0.099, 0.465]                        2.177  [5.96, 3.98, 2.594, 4.386, 4.693]    [4.97, 5.891, 8.891, 7.376, 6.95]       [3.9603960396039604, 3.9603960396039604, 3.9603960396039604, 3.9603960396039604, 3.9405940594059405]  [2.436, 3.168, 5.198, 3.772, 4.01]                           18.584  [4.0, 4.0, 4.0, 4.0, 4.0]                            3.24752                   17.5739                0.39604             14.3689          0.50495                     3.51875                         21.613                    0.712871                 0.00990099             0.0990099           0.00990099               4.46535             0.227723                  2.11881                 1.15842              12.5347                 0                 0                               0                              0                         0                        0                         0                        0                            0                           0                        0                    0                       0                             0                            0                       0                      0                       0                      0                          0                         0                      0\n",
      "Player1                0.017            0.57     33.802           3.237             6.88             2.151    [1.107, 0.167, 0.162, 0.166, 0.0]             0.2255        0.2675        0.432         0.056          0.072     [8.421, 8.723, 10.835, 12.072, 11.256]                              1.518                     6.131                   4.919   [0.306, 0.16, 0.126, 0.115, 0.09]                          0.797  [2.215, 2.24, 2.017, 1.958, 2.123]   [2.47, 2.608, 3.087, 3.449, 3.177]      [3.4595, 3.475, 3.4865, 3.4885, 3.4875]                                                               [2.075, 2.062, 2.707, 2.773, 2.782]                          12.399  [3.511, 3.517, 3.534, 3.54, 3.539]                   2.943                      4.01171               0.853                3.21281         0.8155                      1.03144                         10.553                    0.2045                   0.082                  0.06                0.248                    1.6905              0.14                      0.735                   0.573                 7.1515                 0                 0                               0                              0                         0                        0                         0                        0                            0                           0                        0                    0                       0                             0                            0                       0                      0                       0                      0                          0                         0                      0\n",
      "Player2                0.0185           0.96     34.0445          3.266             6.832            2.112    [1.081, 0.152, 0.169, 0.163, 0.0]             0.2385        0.265         0.4365        0.055          0.082     [8.181, 8.915, 10.231, 12.211, 10.888]                              1.481                     6.183                   5.1695  [0.292, 0.176, 0.142, 0.098, 0.079]                        0.787  [2.184, 2.084, 1.964, 1.81, 2.073]   [2.406, 2.603, 2.933, 3.579, 3.2]       [3.49, 3.5245, 3.526, 3.511, 3.52]                                                                    [2.011, 2.024, 2.615, 2.809, 2.728]                          12.187  [3.535, 3.562, 3.57, 3.558, 3.559]                   2.8855                     4.37937               0.8525               3.69699         0.8075                      1.05845                         10.115                    0.2455                   0.0785                 0.072               0.259                    1.6695              0.145                     0.665                   0.5235                6.8435                 0                 0                               0                              0                         0                        0                         0                        0                            0                           0                        0                    0                       0                             0                            0                       0                      0                       0                      0                          0                         0                      0\n",
      "Player3                0.015            0.57     34.2955          3.2685            6.7915           2.1125   [1.081, 0.154, 0.159, 0.17, 0.0]              0.234         0.276         0.4425        0.0585         0.0705    [8.191, 8.557, 10.691, 11.605, 11.265]                              1.467                     6.2405                  4.9965  [0.266, 0.198, 0.129, 0.137, 0.089]                        0.819  [2.249, 2.204, 1.931, 1.967, 2.092]  [2.402, 2.578, 2.976, 3.366, 3.235]     [3.5075, 3.5095, 3.512, 3.491, 3.485]                                                                 [2.0, 1.976, 2.666, 2.727, 2.744]                            12.113  [3.553, 3.556, 3.559, 3.536, 3.53]                   2.8965                     4.33885               0.851                3.7295          0.8045                      1.02544                         10.443                    0.2055                   0.0855                 0.0665              0.2505                   1.6855              0.151                     0.748                   0.5245                7.0795                 0                 0                               0                              0                         0                        0                         0                        0                            0                           0                        0                    0                       0                             0                            0                       0                      0                       0                      0                          0                         0                      0\n",
      "\n",
      "Num turns: 0\n",
      "\n",
      "\n",
      "Winnings:  [1899, 34, 37, 30]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Running Agent simulations\n",
    "\"\"\"\n",
    "from Agents.AgentRandom2 import AgentRandom2\n",
    "from Agents.AgentMCTS import AgentMCTS\n",
    "from Agents.AgentUCT import AgentUCT\n",
    "from Agents.AgentModel import AgentMultiModel, AgentModel\n",
    "from Game.CatanGame import *\n",
    "from CatanSimulator import CreateGame\n",
    "from DeepLearning.PPO import MaskablePPO\n",
    "from Game.CatanPlayer import PlayerStatsTracker\n",
    "from tabulate import tabulate\n",
    "from DeepLearning.Stats import headers\n",
    "import dill as pickle\n",
    "from CatanData.GameStateViewer import SaveGameStateImage, DisplayImage\n",
    "import math\n",
    "import time\n",
    "\n",
    "winner = [0,0,0,0]\n",
    "player0Stats = PlayerStatsTracker()\n",
    "Player0LosingStats = PlayerStatsTracker()\n",
    "player1Stats = PlayerStatsTracker()\n",
    "player2Stats = PlayerStatsTracker()\n",
    "player3Stats = PlayerStatsTracker()\n",
    "\n",
    "setupModel = MaskablePPO.load(\"DeepLearning/Models/Setup/Setup_Production_23.zip\")\n",
    "\n",
    "\n",
    "testModel0 = MaskablePPO.load(\"DeepLearning/Thesis/Opponents/Models/VsRandomFullObservation/model_6979584_9.zip\")\n",
    "# testModel1 = MaskablePPO.load(\"DeepLearning/Thesis/SparseRewards/Models/Reward_win/Reward_win_10M.zip\")\n",
    "# testModel2 = MaskablePPO.load(\"DeepLearning/Thesis/Opponents/Models/Uniform/model_14667776.zip\")\n",
    "# testModel3 = MaskablePPO.load(\"DeepLearning/Thesis/Opponents/Models/Uniform/model_14667776.zip\")\n",
    "\n",
    "players = [ AgentModel(\"P0\", 0, recordStats=True, playerTrading=False, model=testModel0),\n",
    "            # AgentUCT(\"P1\", 1, recordStats=True, simulationCount=100),\n",
    "            # AgentModel(\"P1\", 1, recordStats=True, playerTrading=False, model=testModel1),\n",
    "            # AgentModel(\"P2\", 2, recordStats=True, playerTrading=False, model=testModel2),\n",
    "            # AgentModel(\"P3\", 3, recordStats=True, playerTrading=False, model=testModel3),\n",
    "            AgentRandom2(\"P1\", 1, recordStats=True, playerTrading=False),\n",
    "            AgentRandom2(\"P2\", 2, recordStats=True, playerTrading=False),\n",
    "            AgentRandom2(\"P3\", 3, recordStats=True, playerTrading=False),\n",
    "            ]\n",
    "\n",
    "\n",
    "COLLECT_STATS = True\n",
    "for episode in range(2000):\n",
    "    game = CreateGame(players)\n",
    "    game = pickle.loads(pickle.dumps(game, -1))\n",
    "    numTurns = 0\n",
    "    while True:\n",
    "        currPlayer = game.gameState.players[game.gameState.currPlayer]\n",
    "\n",
    "        agentAction = currPlayer.DoMove(game)\n",
    "        agentAction.ApplyAction(game.gameState)\n",
    "\n",
    "        # if currPlayer.seatNumber == 0 and agentAction.type == 'EndTurn':\n",
    "        #     DisplayImage(game.gameState, agentAction)\n",
    "        #     time.sleep(0.25)\n",
    "        #     numTurns += 1\n",
    "\n",
    "        if game.gameState.currState == \"OVER\":\n",
    "            # DisplayImage(game.gameState, agentAction)\n",
    "            break\n",
    "    \n",
    "    # print(\"Winner: \", game.gameState.winner)\n",
    "    winner[game.gameState.winner] += 1\n",
    "    lost = game.gameState.winner != 0\n",
    "\n",
    "    # print(winner)\n",
    "\n",
    "    # Stats\n",
    "    if COLLECT_STATS:\n",
    "        game.gameState.players[0].generatePlayerStats()\n",
    "        game.gameState.players[1].generatePlayerStats()\n",
    "        game.gameState.players[2].generatePlayerStats()\n",
    "        game.gameState.players[3].generatePlayerStats()\n",
    "\n",
    "        player0Stats += game.gameState.players[0].stats\n",
    "        player1Stats += game.gameState.players[1].stats\n",
    "        player2Stats += game.gameState.players[2].stats\n",
    "        player3Stats += game.gameState.players[3].stats\n",
    "        if lost:\n",
    "            Player0LosingStats += game.gameState.players[0].stats\n",
    "\n",
    "# Collect stats\n",
    "if COLLECT_STATS:\n",
    "    player0Stats.getAverages()\n",
    "    Player0LosingStats.getAverages()\n",
    "    player1Stats.getAverages()\n",
    "    player2Stats.getAverages()\n",
    "    player3Stats.getAverages()\n",
    "    player0Data = player0Stats.getList()\n",
    "    player0LosingData = Player0LosingStats.getList()\n",
    "    player1Data = player1Stats.getList()\n",
    "    player2Data = player2Stats.getList()\n",
    "    player3Data = player3Stats.getList()\n",
    "\n",
    "    p_hat0 = winner[0] / sum(winner)\n",
    "    p_hat1 = winner[1] / sum(winner)\n",
    "    p_hat2 = winner[0] / sum(winner)\n",
    "    p_hat3 = winner[1] / sum(winner)\n",
    "    margin_error0 = round(100*(1.96 * math.sqrt((p_hat0 * (1 - p_hat0)) / sum(winner))), 2)\n",
    "    margin_error1 = round(100*(1.96 * math.sqrt((p_hat1 * (1 - p_hat1)) / sum(winner))), 2)\n",
    "    margin_error2 = round(100*(1.96 * math.sqrt((p_hat0 * (1 - p_hat0)) / sum(winner))), 2)\n",
    "    margin_error3 = round(100*(1.96 * math.sqrt((p_hat1 * (1 - p_hat1)) / sum(winner))), 2)\n",
    "    player0Data.insert(0, margin_error0)\n",
    "    player0LosingData.insert(0, -1)\n",
    "    player1Data.insert(0, margin_error1)\n",
    "    player2Data.insert(0, margin_error2)\n",
    "    player3Data.insert(0, margin_error3)\n",
    "    player0Data.insert(0, winner[0]/sum(winner))\n",
    "    player0LosingData.insert(0, -1)\n",
    "    player1Data.insert(0, winner[1]/sum(winner))\n",
    "    player2Data.insert(0, winner[2]/sum(winner))\n",
    "    player3Data.insert(0, winner[3]/sum(winner))\n",
    "    player0Data.insert(0, \"Player0\")\n",
    "    player0LosingData.insert(0, \"Player0LossesStats\")\n",
    "    player1Data.insert(0, \"Player1\")\n",
    "    player2Data.insert(0, \"Player2\")\n",
    "    player3Data.insert(0, \"Player3\")\n",
    "\n",
    "    table = tabulate([player0Data, player0LosingData, player1Data, player2Data, player3Data], headers=headers, tablefmt='simple')\n",
    "    print(table)\n",
    "\n",
    "print(f\"\\nNum turns: {numTurns}\")\n",
    "\n",
    "print(\"\\n\\nWinnings: \", winner)\n",
    "\n",
    "\n",
    "# Brick, ore, wool, wheat, wood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Save to csv\n",
    "# fileName = f'DecreasingTurnLimit_v_Random.csv'\n",
    "# df = pd.DataFrame([player0Data, player1Data], columns=headers)\n",
    "# df.to_csv(f'DeepLearning/Thesis/DenseRewards/Data/{fileName}', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
